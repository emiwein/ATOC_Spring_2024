{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMwItiXEIV4u"
   },
   "source": [
    "# ATOC5860 Application Lab #6 - supervised_ML\n",
    "Written by Dr. Middlemas (formerly CU, now PricewaterhouseCoopers) with additional commenting/organizing by Prof. Kay (CU)  \n",
    "Last updated April 18, 2023  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1ig9dv0IV4y"
   },
   "source": [
    "__Learning Goals:__<br>\n",
    "1.\tBuild familiarity with the data processing pipeline (workflow) required to utilize supervised machine learning techniques.\n",
    "2.\tImplement and compare four different supervised learning algorithms \n",
    "3.\tUnderstanding two outcomes of supervised learning algorithms: prediction and feature importance.\n",
    "4.\tStart building a foundation for future machine learning including the following terms: cross-validation, training vs. testing data, metrics (accuracy, recall, precision, f1 score, etc.), overfitting/underfitting, balancing datasets, hyperparameters, & feature importance.  Some future learning resources are provided… but there’s a lot available!  Share resources you find valuable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_r0e1rZIV4y"
   },
   "source": [
    "### DATA and UNDERLYING SCIENCE MOTIVATION:  \n",
    "\n",
    "We will use the Christman dataset which contains weather observations from Fort Collins, Colorado for the year 2016. We will build and train four machine learning models to predict something we already know from the dataset: Is it raining?. The point is not to conduct cutting-edge research or make novel predictions. Instead, the purpose here is to showcase supervised machine learning (ML) models and methods. By the end, we hope you can walk away with more confidence to learn and apply these tools to new problems.\n",
    "\n",
    "Let's say you want to determine which features or atmospheric variables are the best predictors of rainfall. Often, one simply regresses some metric of precipitation onto various atmospheric variables. Then, you assume that whatever returns the highest regression coefficient is the best predictor. While this approach with linear regression presents a fine first guess, it poses a few problems. Linear regression assumes: 1) atmospheric variables are linearly related to precipitation, 2) atmospheric variables are uncorrelated. Yet, both are false assumptions. While a linear relationship between predictor & predictand is a good first guess, why limit yourself to linearity when you can just as easily relax that assumption using supervised Machine Learning...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77aJjuXm5NJi"
   },
   "source": [
    "**Questions to guide your analysis** \n",
    "\n",
    "1) Which machine learning model performs the best to predict rainfall?  What metrics did you use to make this assessment?\n",
    "\n",
    "2) Describe the difference between accuracy and recall.  Why did we choose to use accuracy, recall, and predicted precipitation probability as a way to compare models?  In forecasting: when is a false positive (you said it would rain, it didn’t rain) preferred over a false negative (you said it wouldn’t rain, it did rain)? \n",
    "\n",
    "3) One important \"gotcha\" in a machine learning workflow or pipeline is the order of data preparation. Why should one should perform the train-test split before feature scaling and rebalancing? Hint: think about using a trained model for future predictions. Do you want your scaling of the testing data to depend on the training data? Why perform a test-train split at all? \n",
    "\n",
    "4) Collinearity, or non-zero correlation among features, results in a model that is overly complex, reduces the statistical significance of the fit of the model, and prevents one from correctly identifying the importance of features. Are there features included in our machine learning models to predict rain in the Christman dataset that are collinear? If so, how do you think we should address this collinearity? A couple of suggestions: If we don't have that many features, we could use our meteorological expertise to simply remove one of the features that shares collinearity with other features. Another way to address collinearity is to use feature regularization, or add weights that penalize features that add noise, ultimately reducing model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHnK7nCUIV4z"
   },
   "source": [
    "# Part 1. Read in data into a pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6jkGzBvIV4z"
   },
   "source": [
    "*WARNING: You will need to change the path to match where you have put the Christman dataset on your google drive!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NKczxi_3IV40",
    "outputId": "c9accfb1-ce61-4d4a-ff33-c8d721edf1b6"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      7\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/\u001b[39m\u001b[38;5;124m'\u001b[39m, force_remount\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "5Trskz-u4kEm",
    "outputId": "d5451853-af41-4607-f13b-fa477b56c91b"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/content/drive/MyDrive/ATOC5860/ATOC5860_Spring2023_share/applicationlab6/christman_2016.csv\")\n",
    "\n",
    "df.sample(15) # randomly select & display 15 samples from the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jQF7I_gIV43"
   },
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11LCNI1sIV43"
   },
   "source": [
    "# Part 2. Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNyKMdDQIV43"
   },
   "source": [
    "#### We will be using supervised learning methods to perform the following two tasks:\n",
    "1. Predict the likelihood of rainfall given certain atmospheric conditions\n",
    "    1. Prepping the data\n",
    "    2. Building and training models\n",
    "        1. Logistic regression\n",
    "        2. Random Forest\n",
    "        4. Neural Network\n",
    "2. Determine which variable (\"feature\") is the best predictor of rainfall, i.e., \"feature importance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmJXZApTIV43"
   },
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFCHzUhqIV44"
   },
   "source": [
    "# 2.1. Building a model to predict rainfall likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JT7hU5B9IV44"
   },
   "source": [
    "Let's say you want to determine which features or atmospheric variables are the best predictors of rainfall in Christman, CO. <br><br>\n",
    "Often, one simply regresses some metric of precipitation onto various atmospheric variables, and assume that whatever returns the highest regression coefficient is the best predictor. <br><br>\n",
    "While linear regression presents a fine first guess, it poses a few problems. Linear regression assumes atmospheric variables are linearly related to precipitation, AND that atmospheric variables are not collinear (both of which are false assumptions). <br><br>\n",
    "While a linear relationship between predictor & predictand is a good first guess, why limit yourself to linearity when you can just as easily relax that assumption?<br><br>\n",
    "This is where Machine Learning can come in handy. The rest of this notebook will step through the following Machine Learning model pipeline:<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kj-nAtVFIV44"
   },
   "source": [
    "![model_pipeline](https://drive.google.com/uc?export=view&id=1ZvJXnqUz89UjkbSjUK18QGM0ctQP3b9o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpIEfCviIV44"
   },
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Emufbpt-IV45"
   },
   "source": [
    "# 2.1.B. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiJ3PZWgIV45"
   },
   "source": [
    "Data preparation is a huge part of building Machine Learning model \"pipelines\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOuyAJf4IV45"
   },
   "source": [
    "There are few steps involved when thinking about building & training a Machine Learning model. Remember, models are \"stupid\", and there are a few statistical \"gotcha's\" that may result in your model being biased, inaccurate, or not suitable for the problem at hand. The order of addressing these issues in your data is important (we will address the importance of this order in question 1 at the end)!\n",
    "<br>\n",
    "1. __What exactly are we trying to predict? A value, an outcome, a category? Define your predictors and predictand.__ This requires revisiting your hypothesis or overarching question. In our case, our predictand is the likelihood of precipitation. We will build a Machine Learning model (actually, a few models) that predicts the likelihood that it's currently precipitating at Christman, CO, given current atmospheric conditions. \n",
    "<br><br>\n",
    "2. __Do you have any missing data? If so, how will you handle them? Keep in mind, decreasing the number of input observations may bias your model.__ In our case, we have no missing data.\n",
    "<br><br>\n",
    "3. __Do you have any categorical or non-numeric variables or features? If so, you must figure out how to encode them into numbers.__ Luckily, in the geosciences, we rarely run into this problem.\n",
    "<br><br>\n",
    "4. __How will we validate our model?__ Typically, people split their existing data into training data and testing data, or perform \"__cross-validation__\" or a \"__test-train split__\". That is, we will \"hold out\" some data and call it our \"testing data\", while using the rest of the data to train our model (i.e., \"training data\"). Once our model is trained, we will evaluate its performance with the holdout testing data. *Note:* This could be problematic if there is limited data. \n",
    "<br><br>\n",
    "5. __Do your features have the same variance?__ Just like for K-means clustering above, this can be an important step so that your model doesn't overemphasize one variable with large variance too much. This step is called \"__feature scaling__\". Features of the same size also speed up the Gradient Descent algorithm. For some algorithms, like Linear Regression and Random Forest, feature scaling doesn't matter. But for algorithms like Neural Networks, this becomes very important.\n",
    "<br><br>\n",
    "6. __If classification is the goal, are there the same number of observations for each feature and outcome? If not, how will you rebalance?__ Luckily, we have the same number of observations (8784) for each feature, but a lack of precipitation is way more common than hours with precipitation. We will oversample the observations associated with precip so that the two outcomes (or \"classes\") are equal, though we are making a big assumption: that our subsample is a good representation of each feature distribution. Otherwise, we are building a model that represents unrealistic distributions and thus their respective relationships. *Note: It's important that feature scaling or normalization is performed before any rebalancing so that the qualitative statistics (mean, stddev, etc) remain the same.*\n",
    "<br><br>\n",
    "7. __What are the appropriate metrics for assessing your model? Consider the bias-variance trade-off, and whether having false positives or false negatives is more impactful.__ In our case, predicting no rain when there is rain (false negative) is probably more frustrating and potentially more impactful than the other way around (a false positive). The same can be said for regression models: but metrics computing the model & prediction error are more important (RMSE, R$^2$, MAE, percent error, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaeVA5NoIV45"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOq9yuhVIV45"
   },
   "source": [
    "### Q1. What exactly are you trying to predict?\n",
    "First, split data into predictor & predictands. \n",
    "I'm going to create (or \"engineer\") a new feature that indicates whether precipitation occurred. Clearly, this step only needs to be performed once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9dPGFLTIV46",
    "outputId": "a7e913c1-dff2-42df-bf23-bc8fab0b78a3"
   },
   "outputs": [],
   "source": [
    "df.columns # what is the variable called that indicates precipitation amount?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "3qjxbAtiIV46",
    "outputId": "0e2b3dbe-5a95-4e70-9144-65f8c7467411"
   },
   "outputs": [],
   "source": [
    "df['prec_occur'] = np.array(df.Prec_inches!=0).astype(int)\n",
    "df.sample(15) # randomly select & display 15 samples from the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_LLK4ZVIV46"
   },
   "source": [
    "Then, select the data that will be predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "4t_N2kF8IV46",
    "outputId": "da945137-e1df-48ec-9171-440f5cf197f3"
   },
   "outputs": [],
   "source": [
    "predictors = df.copy(deep=True) \n",
    "# deep = True so that changes to predictors won't be made to original df.\n",
    "\n",
    "# these shouldn't go into predicting whether or not there is rain.\n",
    "predictors = df.drop(['day','hour','Prec_inches'],axis=1) \n",
    "predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCkngJDHIV46"
   },
   "source": [
    "Great, that worked. Now I will assign everything but \"prec\" to be the predictor array \"x\", and prec will be the predictand vector \"y\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "QwfQueR_IV46",
    "outputId": "eaf7792c-ec64-4b07-a0b6-bce7234dd84a"
   },
   "outputs": [],
   "source": [
    "x = predictors.drop('prec_occur',axis=1)\n",
    "y = predictors.prec_occur\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lCuzPWb2NqCR",
    "outputId": "b1f3e9b5-0135-4ed4-ff4c-788ad290ad59"
   },
   "outputs": [],
   "source": [
    "y # should be a pandas series (type: pd.Series) - this or numpy.1DArray is usually the type required for scikit-learn.\n",
    "  # be careful if you use a pd.Series because the index becomes important! Indexing can be useful but has also caused me many headaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Mml7XyMIV47"
   },
   "source": [
    "### Q2 & Q3 do not need to be addressed in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaA1qjHLIV47"
   },
   "source": [
    "### Q4. How will you validate your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1iW4B3wIV47"
   },
   "source": [
    "Next, perform a test-train split to ensure we can validate our trained model. <br>\n",
    "\n",
    "This step must be performed before each time the model is trained to ensure we are not baking in any bias among the models we train. That also means the following two steps must also be performed prior to training each model as well. For this reason, I wrote functions to call easily before each model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJS06XrpH3Gt"
   },
   "source": [
    "*__Note__: Because we randomize our selection of data that we use to train the models, you may find slightly different model metrics, and thus, different answers than your neighbor. To circumference this and compare notes, input an integer for the variable* `random_state` *. Right now, it randomly selects one between 0 and 1000, but it can be any integer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5-I4BfHIV47"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ew_mEfURIV47"
   },
   "outputs": [],
   "source": [
    "def define_holdout_data(\n",
    "    x: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    verbose: bool, \n",
    "    random_state: int=\"\") -> tuple:\n",
    "    \"\"\"Perform a 80/20 test-train split (80% of data is training, 20% is \n",
    "    testing). Split is randomized with each call.\n",
    "    \n",
    "    :param x: a pandas dataframe containing all observations of the predictors\n",
    "    of the dataset.\n",
    "    :param y: a pandas series containing all observations of the predictands\n",
    "    in the dataset.\n",
    "    :param verbose: a boolean that is True if the user would like to print the\n",
    "    variable shapes of the resulting datasets after splitting.\n",
    "    :param random_state: (optional) an integer indicating the `random_state`\n",
    "    of the splitting of the dataset. If not provided, some integer between 0\n",
    "    and 1000 will randomly be chosen. User should provide if replicability\n",
    "    is desired.\n",
    "\n",
    "    :return: a tuple containing x & y, but split into training and testing\n",
    "    data, respectively. (x_train, x_test, y_train, y_test)\n",
    "    \n",
    "    \"\"\"\n",
    "    if not random_state:\n",
    "      random_state = randint(0,1000)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20,\n",
    "                                                        random_state=random_state)\n",
    "    if verbose==True:\n",
    "        print(\"Prior to scaling and rebalacing...\")\n",
    "        print(\"Shape of training predictors: \"+str(np.shape(x_train)))\n",
    "        print(\"Shape of testing predictors: \"+str(np.shape(x_test)))\n",
    "        print(\"Shape of training predictands: \"+str(np.shape(y_train)))\n",
    "        print(\"Shape of testing predictands: \"+str(np.shape(y_test)))\n",
    "        print(\" \")\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SKDoFr8IV47"
   },
   "source": [
    "### Q5. Do your features have the same variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZF143eDAIV48"
   },
   "source": [
    "Now we must normalize the features, or perform \"__Feature Scaling__\", for the same reasons why we normalized prior to running the K-Means clustering algorithm. \n",
    "  \n",
    "The difference here is that I will keep the data as a pandas dataframe rather than converting it to a numpy array beforehand. The \"fit_transform\" function outputs a numpy array, but we will convert back to a dataframe so that re-balancing the dataset is easier. \n",
    "  \n",
    "*__Note__*: If my predictand wasn't binary, then I would also want to normalize that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dr8d0j1pIV48"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2M6shVpbIV48"
   },
   "outputs": [],
   "source": [
    "def scale_data(x_train: pd.DataFrame, x_test: pd.DataFrame) -> tuple:\n",
    "    \"\"\"\n",
    "    Scale training data so that model reaches optimized weights much faster. \n",
    "    Scaling could mean standardizing (so that the standard deviation is 1) or \n",
    "    normalizing (the max and min are 1 and -1). Here we normalize.\n",
    "    \n",
    "    *All data that enters the model should use the same scaling used to scale \n",
    "    the training data.*\n",
    "    Thus, we also perform scaling on testing data for validation later. \n",
    "    Additionally, we return the scalar used to scale any other future input \n",
    "    data.\n",
    "\n",
    "    :param x_train: a pandas dataframe containing observations in the training\n",
    "    dataset. Should have the same columns as `x_test`\n",
    "    :param x_test: a pandas dataframe containing observations in the testing\n",
    "    dataset. Should have the same columns as `x_train`\n",
    "    :return: a tuple containing a MinMaxScaler object that will scale future \n",
    "    datasets in the same way, the scaled predictors from the training and \n",
    "    testing datasets (scaler, x_train_scaled, x_test_scaled).\n",
    "    \"\"\"\n",
    "    \n",
    "    scaler = preprocessing.MinMaxScaler() # normalize \n",
    "    x_train_scaled = pd.DataFrame(data=scaler.fit_transform(x_train),index=x_train.index,columns=x_train.columns) \n",
    "    x_test_scaled = pd.DataFrame(data=scaler.transform(x_test),index=x_test.index,columns=x_test.columns)\n",
    "    \n",
    "    return scaler, x_train_scaled, x_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HKlvh1LIV48"
   },
   "source": [
    "### Q6. Are there the same number of observations for each outcome or class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42dQJiGmIV48"
   },
   "source": [
    "Luckily, we have the same number of observations for each feature (8784). But do we have the same number of outcomes for our predictand - i.e., the same number of hours that are precipitating as those that are non-precipitating?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "knGfVRuOIV48",
    "outputId": "18827cb9-2ac7-4261-be3d-c306813b193b"
   },
   "outputs": [],
   "source": [
    "df['prec_occur'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUhQm_TpIV48"
   },
   "source": [
    "Definitely not. The outcomes we are trying to predict are extremely unbalanced. Non-precip hours occur 30x more than precip hours. This class imbalance may bias the model because precip hours are underrepresented, which means the model won't have as many instances of precip hours to learn to distinguish precip hours from non-precip hours. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXrYnLPlIV49"
   },
   "source": [
    "There are a number of out-of-the-box functions that resample data very precisely. The one I use below simply randomly copies the existing precipitating observation data to balance the dataset. <br><br>\n",
    "There are a couple of risks associated with overfitting. For example, if your sample isn't a good representation of the larger distribution, you could be falsely representing relationships between samples. Additionally, if your minority case (i.e., precip days) are very rare, oversampling may increase the likelihood of occurring overfitting, since it makes copies of the minority class examples. Please see the [Wikipedia page](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis) for some more guidance in your future Google search on resampling data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmgSSdNDIV49"
   },
   "source": [
    "*Note*: This function should be called on both training and testing data separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aoUEtoldIV49"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8z0RTo3IV49"
   },
   "outputs": [],
   "source": [
    "def balance_data(\n",
    "    x: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    verbose: bool,\n",
    "    random_state:int = \"\") -> tuple:\n",
    "    \"\"\"\n",
    "    Resample data ensure model is not biased towards a particular outcome of \n",
    "    precip or no precip.\n",
    "    \n",
    "    :param x: a pandas dataframe with rows of observations and columns \n",
    "    corresponding to predictors of rainfall.\n",
    "    :param y: a pandas series indicating whether rainfall is happening.\n",
    "    :param verbose: a boolean indicating whether printing resulting balanced\n",
    "    dataset shapes is desired.\n",
    "    :param random_state: (optional) an integer indicating the `random_state`\n",
    "    of the splitting of the dataset. If not provided, some integer between 0\n",
    "    and 1000 will randomly be chosen. User should provide if replicability\n",
    "    is desired.\n",
    "\n",
    "    :return: a tuple containing predictors and predictands with balanced \n",
    "    majority & minority classes.\n",
    "    \"\"\"\n",
    "    # Combine again to one dataframe to ensure both the predictor and predictand are resampled from the same \n",
    "    # observations based on predictand outcomes. \n",
    "    dataset = pd.concat([x, y],axis=1)\n",
    "\n",
    "    # Separating classes\n",
    "    raining = dataset[dataset['prec_occur'] == 1]\n",
    "    not_raining = dataset[dataset['prec_occur'] == 0]\n",
    "\n",
    "    if not random_state:\n",
    "      random_state = randint(0,1000)\n",
    "    oversample = resample(raining, \n",
    "                           replace=True, \n",
    "                           n_samples=len(not_raining), #set the number of samples to equal the number of the majority class\n",
    "                           random_state=random_state)\n",
    "\n",
    "    # Returning to new training set\n",
    "    oversample_dataset = pd.concat([not_raining, oversample])\n",
    "\n",
    "    # reseparate oversampled data into X and y sets\n",
    "    x_bal = oversample_dataset.drop(['prec_occur'], axis=1)\n",
    "    y_bal = oversample_dataset['prec_occur']\n",
    "\n",
    "    if verbose==True:\n",
    "        print(\"After scaling and rebalacing...\")\n",
    "        print(\"Shape of predictors: \"+str(np.shape(x_bal)))\n",
    "        print(\"Shape of predictands: \"+str(np.shape(y_bal)))\n",
    "        print(\" \")\n",
    "    \n",
    "    return x_bal, y_bal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tC49gaz_IV49"
   },
   "source": [
    "#### Now, let's put the data prep code from questions 1-6 into a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gmEsLaXTIV49"
   },
   "outputs": [],
   "source": [
    "def dataprep_pipeline(\n",
    "    x: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    verbose: bool,\n",
    "    random_state: int=\"\") -> tuple:\n",
    "    \"\"\"\n",
    "    Combines all the functions defined above so that the user only has to \n",
    "    call one function to do all data pre-processing.\n",
    "    \n",
    "    :param x: a pandas dataframe with rows of observations and columns \n",
    "    corresponding to predictors of rainfall.\n",
    "    :param y: a pandas series indicating whether rainfall is happening.\n",
    "    :param verbose: a boolean indicating whether printing algorithm status \n",
    "    updates are desired.\n",
    "\n",
    "    :return: a tuple containing training & testing x & y dataframes & series,\n",
    "    respectively - but minority and majority classes now balanced.\n",
    "\n",
    "    \"\"\"\n",
    "    # if no random_state is provided, choose one:\n",
    "    if not random_state:\n",
    "      random_state = randint(0,1000)\n",
    "\n",
    "    # split into training & testing data\n",
    "    if verbose==True:\n",
    "        print(\"Defining holdout data... \")\n",
    "    x_train, x_test, y_train, y_test = define_holdout_data(x, y, verbose,\n",
    "                                                           random_state) \n",
    "\n",
    "    # perform feature scaling\n",
    "    if verbose==True:\n",
    "        print(\"Performing feature scaling... \")\n",
    "    scaler, x_train_scaled, x_test_scaled = scale_data(x_train, x_test)\n",
    "\n",
    "    # rebalance according to outcomes (i.e., the number of precipitating \n",
    "    # observations & non-precipitating outcomes should be equal)\n",
    "    if verbose==True:\n",
    "        print(\"for training data... \")\n",
    "    x_train_bal, y_train_bal = balance_data(x_train_scaled, y_train, verbose, random_state)\n",
    "    if verbose==True:\n",
    "        print(\"for testing data... \")\n",
    "    x_test_bal, y_test_bal = balance_data(x_test_scaled, y_test, verbose, random_state)\n",
    "\n",
    "    if len(y_train_bal)<len(y):\n",
    "      raise Exception(\"Something's not right. Oversampling somehow resulted in a smaller dataset.\")\n",
    "    \n",
    "    return x_train_bal, y_train_bal, x_test_bal, y_test_bal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WkH6hq78WyQK",
    "outputId": "a49c25ef-c7eb-4310-945a-b98caf62fb67"
   },
   "outputs": [],
   "source": [
    "x_train_bal, y_train_bal, x_test_bal, y_test_bal = dataprep_pipeline(x, y, verbose=True, random_state=80305)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FbtTw03XW0CQ",
    "outputId": "11c00ee1-b720-4c87-8ab0-1ed26ff3074f"
   },
   "outputs": [],
   "source": [
    "x_train_bal2, y_train_bal2, x_test_bal2, y_test_bal2 = dataprep_pipeline(x, y, verbose=True, random_state=80305)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsW0VLltIV49"
   },
   "source": [
    "### Q7. What are the appropriate metrics for assessing your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2K0WmhLzIV49"
   },
   "source": [
    "These metrics will be used to evaluate the model after training. Thus, these functions will also be called for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NbyJvSEIV4-"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkK9ZhjpIV4-"
   },
   "source": [
    "#### First, define functions for evaluating models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75F6-jmlIV4-"
   },
   "source": [
    "I'm simply going to train various models and then look at their model metrics. Then we can make a prediction based on some selected observation to obtain a likelihood that it's raining, and we can compare with our own eyes whether the model gets it right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KT-SM4GxIV4-"
   },
   "source": [
    "Below are some commonly-used metrics for assessing the value of a given Machine Learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QB7XFBKoIV4-"
   },
   "source": [
    "\"**True Positive (TP)**\" Is the number of times the model predicts a positive when the observation is actually positive. In our case, the model predicts that its raining when it is actually raining.<br>\n",
    "\"**False Positive (FP)**\" The number of times the model guesses that it's raining when it's not actually raining.<br>\n",
    "The same applies to **True Negatives (TN)** (correctly predicting that it's not raining) and **False Negatives (FN)** (predicting no rain when it's actually raining).\n",
    "\n",
    "\n",
    " - **Precision = TP/(TP + FP)**: The proportion of predicted precipitating events that are actually precipitating.\n",
    " - **Accuracy = (TP + TN)/(total)**: The proportion of precipitating hours or non-precipitating hours that are correctly predicted by the model.\n",
    " - **Recall = TP/(TP + FN)**: The proportion of precipitating hours that are correctly predicted by the model.<br>\n",
    "<br>\n",
    "Other important metrics that we aren't going to look at today:\n",
    " - **F1**: a way to capture how well the model predicts the hours that it's actually precipitating.\n",
    " - **ROC/AUC**: how well the model separates precipitating hours from non-precipitating hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvvpAciWIV4-"
   },
   "source": [
    "Below, I define a couple of functions that will help us visualize the metrics listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPaTeTm4IV4-"
   },
   "outputs": [],
   "source": [
    "# Print rounded metrics for each model.\n",
    "def bin_metrics(y_hat: pd.Series, y: pd.Series) -> tuple:\n",
    "    \"\"\"Prints accuracy and recall metrics for evaluating classification \n",
    "    predictions.\n",
    "    \n",
    "    :param y_hat: a pandas series that contains some boolean indication of \n",
    "    whether it's raining or not predicted by the trained model. \n",
    "    :param y: a pandas series that contains some boolean indication of \n",
    "    whether it's raining or not from observations\n",
    "\n",
    "    :return: a tuple containing values for the accuracy & recall, respectively.\n",
    "    \"\"\"\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(y_hat, y)\n",
    "    recall = metrics.recall_score(y_hat, y)\n",
    "\n",
    "    print('Accuracy:', round(accuracy, 4))\n",
    "    print('Recall:', round(recall, 4))\n",
    "    \n",
    "    return accuracy, recall\n",
    "\n",
    "\n",
    "# Pplot confusion matrix\n",
    "def plot_cm(y_hat: pd.Series, y: pd.Series) -> None:\n",
    "    \"\"\"Plots the confusion matrix to visualize true \n",
    "    & false positives & negatives\n",
    "\n",
    "    :param y_hat: a pandas series that contains some boolean indication of \n",
    "    whether it's raining or not predicted by the trained model. \n",
    "    :param y: a pandas series that contains some boolean indication of \n",
    "    whether it's raining or not from observations\n",
    "\n",
    "    :return: None    \n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_hat, y)\n",
    "    df_cm = pd.DataFrame(cm, columns=np.unique(y_hat), index = np.unique(y_hat))\n",
    "    df_cm.index.name = 'Actual'\n",
    "    df_cm.columns.name = 'Predicted'\n",
    "    sns.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 25}, fmt='g')# font size\n",
    "    plt.ylim([0, 2])\n",
    "    plt.xticks([0.5, 1.5], ['Negatives','Positives'])\n",
    "    plt.yticks([0.5, 1.5], ['Negatives','Positives'])\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31uH2ic4IV4-"
   },
   "source": [
    "Based on our answer to number 7 under the \"1. Data Preparation\" section above, I will be using **Accuracy** and **Recall** to compare these four different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RStOBm2OIV4_"
   },
   "source": [
    "#### Another way we can evaluate the models is to compare precipitation likelihood given the same set of atmospheric conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ae7XptvuIV4_"
   },
   "source": [
    "First, let's choose some observation in the pre-scaled dataset shows that it's raining, and then find the corresponding scaled observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wrUkEgo3IV4_"
   },
   "outputs": [],
   "source": [
    "def rand_atmos_conditions_precip(\n",
    "    random_state: int=randint(0,1000),\n",
    "    index: str='rand') -> tuple:\n",
    "    \"\"\"\n",
    "    Function returns atmospheric conditions in a dataframe as well as the scaled\n",
    "    conditions in a numpy array so that they output a prediction in the model.\n",
    "    \n",
    "    If no input is passed, the function will randomly generate an in index to \n",
    "    choose from those observations in some training data with precipitation. \n",
    "    Otherwise, an integer index between 0 and 200 should be passed.\n",
    "\n",
    "    :param random_state:\n",
    "    :param index:\n",
    "    \"\"\"\n",
    "    # First, perform a test-train split\n",
    "    x_train, x_test, y_train, _ = define_holdout_data(x, y, verbose=False) \n",
    "\n",
    "    # perform feature scaling - this happens *AFTER* the test-train split\n",
    "    _, x_train_scaled, _ = scale_data(x_train, x_test)\n",
    "\n",
    "    # this is what will go into the model to output a prediction\n",
    "    if index=='rand':\n",
    "        index = randint(0,len(y_train[y_train==1].index)) \n",
    "    precipindex = y_train[y_train==1].index.values[index]\n",
    "    testpredictor = x_train_scaled.loc[precipindex] \n",
    "    \n",
    "    return df.iloc[precipindex], testpredictor    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8MbicKjIV4_"
   },
   "source": [
    "I will choose some atmospheric conditions after training the first model, and then use those conditions for the other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrR8mY6ZIV4_"
   },
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FL68d-Q0IV4_"
   },
   "source": [
    "# 2.1.B Train & compare four ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zkd-8UTCIV4_"
   },
   "source": [
    "Each section below goes through building and training a ML model. In each section, there are a few steps for each model \"pipeline\":\n",
    "1. __Randomly perform a test-train split, feature scaling, and resample data to ensure outcomes are balanced__. \n",
    "2. __Train your model__.\n",
    "3. __Assess model metrics with testing and training data__. We begin by first assessing each model's performance by calculating the metrics defined above on the *testing* or *holdout* data; the key here is that the model has never seen this data. <br>__If applicable, tune your model.__ This means choosing new *hyperparameters*, retraining the model, and then reassessing the same model metrics to see if the model yields better results.\n",
    "3. __Check for model overfitting__. We will also check to see if the model is overfitting by comparing metrics of the testing data to that of the training data. In short, the training data should not be outperforming the testing data.\n",
    "4. __Actually make a prediction with a single observation__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SB9KLMQlIV4_"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Bgo853jIV4_"
   },
   "source": [
    "## 2.1.B.a. [Logistic Regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xi9mBlyhIV5A"
   },
   "source": [
    "Let's just start with a simple Logistic Regression. A Logistic Regression is a very specialized model in that in that it only computes the probability of a binary outcome.  \n",
    "![logistic regression gif](https://drive.google.com/uc?export=view&id=1_LOP_Vdi5DkICWRDf7tgN-OMMTRGMQRm)\n",
    "<br>\n",
    "This model also requires that the assumptions of linear regression must hold, i.e., <br>\n",
    "1. linear relationship between predictors & predictand, \n",
    "2. normally distributed residuals between observed predictand and predicted outcome, \n",
    "3. no collinearity must exist among predictors, i.e., must be independent<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6MvvpxXIV5A"
   },
   "source": [
    "Logistic Regression shouldn't perform any better than a linear regression, so let's just start with this as an illustration of how well linearity assumptions hold in this dataset.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "noAxbS4BIV5A"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1RX7rBBIV5A"
   },
   "source": [
    "#### 1. Perform a test-train split, perform feature scaling, and the rebalance our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0wzanzxuIV5A",
    "outputId": "b841f981-7113-42b7-9040-983c28be532a"
   },
   "outputs": [],
   "source": [
    "x_train_bal, y_train_bal, x_test_bal, y_test_bal = dataprep_pipeline(x, y, verbose=True, random_state=80305)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWAiRkaQIV5A"
   },
   "source": [
    "#### 2. Train the Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EgsWVLy7IV5A"
   },
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "lr = LogisticRegression(solver='lbfgs') \n",
    "# we choose this particular solver because we're not regularizing or penalizing certain features\n",
    "\n",
    "# fit the model to scaled & balanced training data. Side note: this is where *Gradient Descent* occurs.\n",
    "lr.fit(x_train_bal, y_train_bal);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38XT6zGvIV5A"
   },
   "source": [
    "#### 3. Assess Logistic Regression's performance using testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEykWryGIV5A"
   },
   "source": [
    "Now that we've \"trained\" our model, we make predictions using data that the model has never seen before (i.e., our holdout testing data) to see how it performs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "gKfIHcmpIV5B",
    "outputId": "b71d988c-4f25-4980-e005-75ed538a328e"
   },
   "outputs": [],
   "source": [
    "y_pred = lr.predict(x_test_bal)\n",
    "\n",
    "# Call functions defined above to calculate metrics & plot a confusion matrix based on\n",
    "# how well model simulates testing data\n",
    "plot_cm(y_test_bal, y_pred);\n",
    "lr_acc, lr_rec = bin_metrics(y_test_bal, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xJ5PrsBIV5B"
   },
   "source": [
    "We determined earlier that False Positives are less harmful than False Negatives. Thus, along with accuracy, which essentially measures how close the model is to the target, we should try to maximize recall. Luckily, recall is the highest-scoring metric for this model (recall = ~87%).<br><br>\n",
    "Let's focus on accuracy, though, for a moment. Accuracy tells is the percent of correct predictions, whether precipitating or not. The Logistic Regression model, without any additional tuning, can correctly predict whether it's precipitating or not given a set of present atmospheric conditions around 84% of the time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxHrK_07IV5B"
   },
   "source": [
    "#### 4. Check to see if the Logistic Regression model is overfitting (or underfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrRFeanxIV5B"
   },
   "source": [
    "A very important aspect of tuning machine learning model is to ensure the model isn't overfitting or underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKifLlZbIV5B"
   },
   "source": [
    "![overfitting.v.underfitting](https://drive.google.com/uc?export=view&id=1cKNtqszFpzcLxPnNhSwHwvK_W_YGLfs1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiZMjC5xIV5B"
   },
   "source": [
    "An underfit model is a biased model. Symptoms of underfitting are that your training data performs worse than your testing data, or the model metrics are just poor overall. To alleviate underfitting, introduce more variance into the model by adding more features, use ensemble methods, or increase model complexity. <br><br>\n",
    "An overfit model means the model is fit very well to the training data, but fails to generalize predictions outside the training dataset. A symptom of overfitting is that the models' training accuracy is much better than the testing accuracy. Overfitting can happen more easily in more complex models, like neural networks. To alleviate overfitting, one needs to reduce variance, through **feature regularization**, lowering model complexity, or performing k-folds cross-validation.<br><br>\n",
    "Before you dive too deeply into ML and in your own time, I suggest watching [this 6-minute StatQuest YouTube video](https://www.youtube.com/watch?v=EuBBz3bI-aA) to develop an intuition of error, overfitting, and underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PenIuu-GIV5B",
    "outputId": "45f18e30-775e-4499-fb45-cfb44177fbc8"
   },
   "outputs": [],
   "source": [
    "# Compare testing data metrics to data training metrics.\n",
    "print(\"Training metrics:\")\n",
    "pred_train= lr.predict(x_train_bal) \n",
    "bin_metrics(y_train_bal,pred_train);\n",
    "\n",
    "# As a reminder, display testing metrics:\n",
    "print(\" \")\n",
    "print(\"Testing metrics:\")\n",
    "bin_metrics(y_test_bal, y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0_ar58qIV5B"
   },
   "source": [
    "Remember:<br>\n",
    "testing metrics > training metrics = underfitting, model is too simple<br>\n",
    "testing metrics < training metrics = overfitting, model is too complex<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFuoUxvIIV5B"
   },
   "source": [
    "#### 5. Make a prediction with the Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7aJT3MvSIV5B"
   },
   "source": [
    "First, we randomly choose some atmospheric conditions using the function defined above. This will be the atmospheric conditions we use for all models we build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTzj5layIV5C"
   },
   "outputs": [],
   "source": [
    "origvals, testpredictor = rand_atmos_conditions_precip()\n",
    "# print(origvals) # observation from original dataframe\n",
    "# print(testpredictor) # scaled observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ap69nuemIV5C",
    "outputId": "af1152e0-98bb-4b5d-c712-a377515e73a0"
   },
   "outputs": [],
   "source": [
    "# prediction output is in the format [probability no rain, probability rain]\n",
    "lr_prediction = lr.predict_proba(np.array(testpredictor).reshape(1, -1))[0][1]*100 \n",
    "print(\"The meteorological conditions are: \")\n",
    "print(origvals)\n",
    "print(\" \")\n",
    "print(f\"There is a {round(lr_prediction,2)}% chance of precipitation given those meteorological conditions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKpVlf8hIV5C"
   },
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLGGhznpIV5C"
   },
   "source": [
    "## 2.1.B.b. [Random Forest](https://scikit-learn.org/stable/modules/ensemble.html#forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gunv0UuVIV5C"
   },
   "source": [
    "To understand random forests, one must first understand a [decision tree](https://scikit-learn.org/stable/modules/tree.html#tree). A decision tree is intuitive: it is essentially a flowchart to point to an outcome based on \"decisions\" for each feature. <br><br>\n",
    "A Random Forest is an ensemble of decision trees that are randomly constructed based on the features of the dataset and number of decisions. Trees are constructed by randomly choosing a feature to \"seed\" each tree, and then making rules or associations with other features to lead to the specified outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBWcKG0MIV5C"
   },
   "source": [
    "![random_forest](https://drive.google.com/uc?export=view&id=1Epldg87IF1Tu9-wTGOe5Rpp0Jvewwmrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tg2Euy65IV5C"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKEwvM4KIV5C"
   },
   "source": [
    "#### 1. If you didn't perform for Logistic Regression above, perform a test-train split, perform feature scaling, and the rebalance our dataset. \n",
    "Otherwise, you can run this cell, but since I've specified the `random_state` to be the same as above, you should get the same set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYhHN-NIIV5C"
   },
   "outputs": [],
   "source": [
    "if 'x_train_bal' not in locals():\n",
    "  x_train_bal, y_train_bal, x_test_bal, y_test_bal = dataprep_pipeline(x, y, verbose=False, random_state=80305)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4j2h_GB6IV5D"
   },
   "source": [
    "#### 2. Train (and tuning) the Random Forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRAz1BzKIV5D"
   },
   "source": [
    "#### Choosing hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xEzk2hxIV5D"
   },
   "source": [
    "Typically, in Machine Learning models, we can choose a hyperparameters that control the behavior of the model and can improve model performance. These hyperparamters are just numbers that tell the model to act in different ways. Adjusting hyperparameters can control the complexity of the model, and thus, influence model metrics. <br><br>\n",
    "There are many hyperparameters one can decide upon when tuning the Random Forest classifier (see scikit-learn page on Random Forests for more examples). The two we will be adjusting is \n",
    "1. The number of estimators or \"trees\" in the forest\n",
    "2. The depth of the tree, or how many \"decisions\" are made until convergence is reached. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1bLPGHc5IV5D",
    "outputId": "a5676658-0257-4518-ebf1-e480c65667ec"
   },
   "outputs": [],
   "source": [
    "acc_scores = []\n",
    "rec_scores = []\n",
    "\n",
    "num_est = [10, 50, 500] # number of trees\n",
    "depth = [2, 10, 100] # number of decisions\n",
    "for i in num_est:\n",
    "    start = time.time()\n",
    "    print(\"Number of estimators is \"+str(i))\n",
    "\n",
    "    for k in depth:\n",
    "        print(\"depth is \"+str(k))\n",
    "        forest = RandomForestClassifier(n_estimators=i, max_depth=k)\n",
    "        forest.fit(x_train_bal, y_train_bal)\n",
    "        \n",
    "        # cross validate & evaluate metrics based on testing data\n",
    "        pred_test= forest.predict(x_test_bal)\n",
    "        acc_val = metrics.accuracy_score(y_test_bal, pred_test)\n",
    "        acc_scores.append(acc_val)\n",
    "        rec_val = metrics.recall_score(y_test_bal, pred_test)\n",
    "        rec_scores.append(rec_val)\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Random Forest took \"+str(end-start)+\" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "gR8Nu5MXIV5D",
    "outputId": "5d55bd21-7ed4-4d2a-d30b-aed41970daca"
   },
   "outputs": [],
   "source": [
    "plt.plot(acc_scores, marker='o', color='black')\n",
    "plt.plot(rec_scores, marker='o', color='blue')\n",
    "print(\"Max Accuracy (black):\", round(max(acc_scores), 4))\n",
    "print(\"Max Recall (blue):\", round(max(rec_scores), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rVW4i3hIV5D"
   },
   "source": [
    "**x =** <br>\n",
    "0, 1, 2: number of estimators = 10<br>\n",
    "3, 4, 5: number of estimators = 50<br>\n",
    "6, 7, 8: number of estimators = 500<br>\n",
    "<br>\n",
    "0, 3, 6: depth range = 2<br>\n",
    "1, 4, 7: depth range = 10<br>\n",
    "2, 5, 8: depth range = 100<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UlKy7zgIV5D"
   },
   "source": [
    "Choosing the right hyperparameters for this model requires revisiting which metrics are most important to our question. We want to maximize both recall and accuracy. <br><br>\n",
    "For simplicity, I choose the parameters corresponding to x=0, but __I highly suggest you try other combinations of parameters as well__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BgZvDHMBIV5D"
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=10, max_depth=2);\n",
    "forest.fit(x_train_bal, y_train_bal);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwDBiAkVIV5D"
   },
   "source": [
    "#### 3. Assess the Random Forest's performance using testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwT15zu2IV5E"
   },
   "source": [
    "Once again, we will use our testing data to make an initial evaluation of how the model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "id": "ObvARYd8IV5E",
    "outputId": "565cc5bf-97ba-4d50-c8af-2d00323c2a77"
   },
   "outputs": [],
   "source": [
    "pred_test= forest.predict(x_test_bal)\n",
    "\n",
    "# Call functions defined above to calculate metrics & plot a confusion matrix based on\n",
    "# how well model simulates testing data\n",
    "forest_acc, forest_rec = bin_metrics(y_test_bal, pred_test)\n",
    "plot_cm(y_test_bal, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D87thTfXIV5E"
   },
   "source": [
    "#### 4. Check to see if the Random Forest is overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KCcyA9I4IV5E",
    "outputId": "24102bbd-cca1-4123-aae4-9b4b958f938b"
   },
   "outputs": [],
   "source": [
    "# Compare testing data metrics to data training metrics.\n",
    "print(\"Training metrics:\")\n",
    "rf_pred_train= forest.predict(x_train_bal) \n",
    "bin_metrics(y_train_bal,rf_pred_train);\n",
    "\n",
    "# As a reminder, display testing metrics:\n",
    "print(\" \")\n",
    "print(\"Testing metrics:\")\n",
    "bin_metrics(y_test_bal, pred_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huKAdBukIV5E"
   },
   "source": [
    "Remember:<br>\n",
    "testing metrics > training metrics = underfitting, model is too simple<br>\n",
    "testing metrics < training metrics = overfitting, model is too complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1boQogqIV5E"
   },
   "source": [
    "Random forests seldom overfit, but if they do, one should try increasing the number of trees, or decreasing the amount of data used to construct each tree. See scikit-learn's [Random Forest Classifier webpage](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) for information on more hyperparameters one can tune to address overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnSygaSjIV5E"
   },
   "source": [
    "#### 5. Make a prediction with the Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZWCBePvIV5E",
    "outputId": "3fd7da2b-0132-47e0-de3f-2dd3ad20c6ab"
   },
   "outputs": [],
   "source": [
    "# prediction output is in the format [probability no rain, probability rain]\n",
    "forest_prediction = forest.predict_proba(np.array(testpredictor).reshape(1, -1))[0][1]*100 \n",
    "print(\"The meteorological conditions are: \")\n",
    "print(origvals)\n",
    "print(\" \")\n",
    "print(\"There is a {0:.{digits}f}% chance of precipitation given those meteorological conditions.\".format(forest_prediction, digits=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ov_X9yjfIV5E"
   },
   "source": [
    "*The random forest yields yet another percent chance of precipitation given the meteorological conditions... hmm...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oj0Z0iLRIV5E"
   },
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgGja1YnIV5H"
   },
   "source": [
    "## 2.1.B.c. [Neural Network](https://blog.insightdatascience.com/a-quick-introduction-to-vanilla-neural-networks-b0998c6216a1)\n",
    "![neuarl net chart](https://drive.google.com/uc?export=view&id=1dGiM6CMR0kKSh2kVe9N3e_WBdCCzjweq)\n",
    "![neural net equation](https://drive.google.com/uc?export=view&id=1QsWJbcry5SQqalXu3gKwcH9phE98lEOQ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wdnwL9hIV5H"
   },
   "source": [
    "*Note: there is a TON of information online about Neural Networks.* I suggest starting with the 3-part blog series on neural networks accessible in the link above for a gentle introduction to the theory. <br><br>\n",
    "If you'd rather absorb new information through watching a video, I found [this three-part series of youtube videos](https://www.youtube.com/watch?v=aircAruvnKk) (totaling about an hour in length) to be very helpful.<br><br>\n",
    "Another great resource is [machinelearningmastery.com](machinelearningmastery.com). In fact, the model below is based off of [this blog post](https://machinelearningmastery.com/binary-classification-tutorial-with-the-keras-deep-learning-library/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XIXMxG9-IV5H"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDUtulA7IV5H"
   },
   "source": [
    "#### 1. Perform a test-train split, perform feature scaling, and the rebalance our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEwyfrCUIV5H"
   },
   "source": [
    "Split test & training data, perform feature scaling, and rebalance dataset according to outcomes if you didn't do so above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SH_WE9VXIV5I"
   },
   "outputs": [],
   "source": [
    "if 'x_train_bal' not in locals():\n",
    "  x_train_bal, y_train_bal, x_test_bal, y_test_bal = dataprep_pipeline(x, y, verbose=False, random_state=80305)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFnhr9XVIV5I"
   },
   "source": [
    "#### 2. Train (and build and compile) the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8XeUH_AIV5I"
   },
   "source": [
    "There are lots of hyperparameters here. Please read the comments to guide you in playing with them later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5a_sRxnIV5I"
   },
   "source": [
    "#### Build a very simple Neural Network & compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-dntfjrIV5I"
   },
   "outputs": [],
   "source": [
    "number_inputs = len(x_train_bal.columns)\n",
    "\n",
    "# create model\n",
    "nn = Sequential()\n",
    "nn.add(Dense(number_inputs, input_dim=number_inputs, activation='relu'))\n",
    "\n",
    "# Try uncommenting this to address overfitting\n",
    "# from keras.regularizers import l2\n",
    "# reg = l2(0.001)\n",
    "# nn.add(Dense(number_inputs, activation='relu',bias_regularizer=reg,activity_regularizer=reg))\n",
    "\n",
    "# try commenting out one and then the other\n",
    "nn.add(Dense(1, activation='sigmoid'))\n",
    "#nn.addDense(1, activation='softmax'))\n",
    "\n",
    "# Compile model \n",
    "# Also try changing the learning rate.\n",
    "learning_rate = 0.001 # only used in the SGD optimizer.\n",
    "\n",
    "# Also try commenting out one & then the other. \n",
    "nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "#nn.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(lr=learning_rate), metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltStUYTjNH_x"
   },
   "source": [
    "*Note: the `binary_crossentropy` is also called the \"Log Loss\". This is a loss function specific for predicting two classes. \n",
    "$$L_{BCE} = -\\frac{1}{n}\\sum{(Y_{i}⋅ log(\\hat{Y_{i}}) + (1-Y_{i})⋅ log(1-\\hat Y_{i})} $$, where $Y_{i}$ and $\\hat{Y_{i}}$ is the actual and the model's predicted probability of one class, respectively.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WokcjjSiIV5I"
   },
   "source": [
    "#### Actually training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B9KJsrjltNTs",
    "outputId": "c72f1c9d-e568-4a29-c360-7298a5587e98"
   },
   "outputs": [],
   "source": [
    "batch_size = 24 # The number of samples the network sees before it backpropagates (batch size) # 24 & 32 yield accuracy = 87%\n",
    "epochs = 100 # The number of times the network will loop through the entire dataset (epochs)\n",
    "shuffle = True # Set whether to shuffle the training data so the model doesn't see it sequentially \n",
    "verbose = 2 # Set whether the model will output information when trained (0 = no output; 2 = output accuracy every epoch)\n",
    "\n",
    "# Train the neural network!\n",
    "start = time.time()\n",
    "\n",
    "history = nn.fit(x_train_bal, y_train_bal, validation_data=(x_test_bal, y_test_bal), \n",
    "          batch_size=batch_size, epochs=epochs, shuffle=shuffle, verbose=verbose)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Neural Network took \"+str(end-start)+\" seconds to train.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIRXvLr7IV5I"
   },
   "source": [
    "#### Accuracy & loss with epochs\n",
    "Neural networks train in __epochs__. During each epoch, the model trains by sweeping over each layer, adjusting weights based on their resulting errors, through processes called __forward propagation__ and __backpropagation__. By plotting the model accuracy & loss which each epoch, we can visualize how the model error evolves with training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "xF9OWn5vIV5J",
    "outputId": "35323ae9-4433-475e-dfe3-ed49db6bd6e5"
   },
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(nrows=2,ncols=1)\n",
    "figure.tight_layout(pad=3.0)\n",
    "\n",
    "# plot accuracy during training\n",
    "plt.subplot(211)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.legend();\n",
    "\n",
    "# plot loss during training\n",
    "plt.subplot(212)\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.xlabel(\"Epoch\");\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vsp6So4rIV5J"
   },
   "source": [
    "#### 3. Assess Neural Network's performance using testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qN1iP-1QIV5J"
   },
   "source": [
    "Though the accuracy is pictured above, additionally quantify recall on testing data with the same functions used previously to remain consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "jdws43tQIV5J",
    "outputId": "766f4334-9609-454e-c8d7-4e7f0d2d8de1"
   },
   "outputs": [],
   "source": [
    "pred_test= (nn.predict(x_test_bal)>0.5).astype(\"int32\")\n",
    "nn_acc, nn_rec = bin_metrics(y_test_bal, pred_test)\n",
    "plot_cm(y_test_bal, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NiEeD7PrIV5J"
   },
   "source": [
    "#### 4. Check to see if the Neural Network is overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0fpB02JIV5J",
    "outputId": "7d3b49f8-f7a0-4b84-d3f9-4edb09f61b2f"
   },
   "outputs": [],
   "source": [
    "# Compare testing data metrics to data training metrics.\n",
    "print(\"Training metrics:\")\n",
    "nn_pred_train= (nn.predict(x_train_bal)>0.5).astype(\"int32\")\n",
    "bin_metrics(y_train_bal,nn_pred_train);\n",
    "\n",
    "# As a reminder, display testing metrics:\n",
    "print(\" \")\n",
    "print(\"Testing metrics:\")\n",
    "bin_metrics(y_test_bal, pred_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HlhM-jeIV5J"
   },
   "source": [
    "Neural networks can easily overfit because they are very complex and can fit to the training data extremely well,  which prevents them from generalizing to other data (like the testing data). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvwMPHH6IV5J"
   },
   "source": [
    "#### 5. Make a prediction with the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EvjRyCL2IV5J",
    "outputId": "f46b834e-855d-4db1-eb88-a38835ff219f"
   },
   "outputs": [],
   "source": [
    "# prediction output is in the format [probability no rain, probability rain]\n",
    "nn_prediction = nn.predict(np.array(testpredictor).reshape(1, -1))[0][0]*100\n",
    "print(\"The meteorological conditions are: \")\n",
    "print(origvals)\n",
    "print(\"There is a {0:.{digits}f}% chance of precipitation given those meteorological conditions.\".format(nn_prediction, digits=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbeXydz-IV5K"
   },
   "source": [
    "*Interesting... How does the Neural Network's estimate of %chance precip compare to that of the other models? See below for final intermodel comparison.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9x9ZXeU9IV5K"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mqIMn6nIV5K"
   },
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y--hxNdPIV5K"
   },
   "source": [
    "# Final intermodel comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "I0uA5ZBHIV5K",
    "outputId": "49206ad1-3987-41dc-90ef-1a85fbdbefae"
   },
   "outputs": [],
   "source": [
    "model_metrics = pd.DataFrame({'Metrics':['Accuracy','Recall','Prediction example'],\n",
    "    'Logistic Regression':[lr_acc, lr_rec, lr_prediction],\n",
    "    'Random Forest':[forest_acc, forest_rec, forest_prediction],\n",
    "    'Neural Network':[nn_acc, nn_rec, nn_prediction]})\n",
    "model_metrics = model_metrics.set_index('Metrics')\n",
    "model_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NH85_804IV5K"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32-_q9y8IV5K"
   },
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIalmxJiIV5K"
   },
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tBTst1_IV5K"
   },
   "source": [
    "The code below shows the way one would identify the most important features in each model, i.e., the best predictors of rainfall. The code looks a little different from model to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRN5ORkGIV5K"
   },
   "outputs": [],
   "source": [
    "# If you want to look at the pre-scaled data used in the models, uncomment the line below\n",
    "#x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7f2SHGGIV5K"
   },
   "source": [
    "#### Feature Importance in Logistic Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMCCxOUpTkxH"
   },
   "source": [
    "Luckily, feature importance in linear models is usually quite easy to obtain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "x6vpbLsTIV5L",
    "outputId": "5997e302-3c78-41bf-ce04-7fc3695216ae"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(abs(lr.coef_[0]),\n",
    "             index = x.columns,\n",
    "             columns=['importance']).sort_values('importance',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uavgPQ2lIV5L"
   },
   "source": [
    "#### Finding most important features in a Random Forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yA5i420aRQD-"
   },
   "source": [
    "There are a number of different ways to calculate feature importance in tree-based algorithms. We are using the default feature importance algorithm here found in the Random Forest package, which are **Gini importance**. In some instances, [Gini Importance can falseley put too much importance on some features if the model is overfitting](https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html). Some find that **permutation importance** or, better yet, [**SHAP values**](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html) are a more robust way to derive feature importances (albeit more costly and perhaps a little more difficult to understand). <br><br>\n",
    "*Side note: the site linked above that documents the python package for calculating SHAP values [provides a nice warning](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/Be%20careful%20when%20interpreting%20predictive%20models%20in%20search%20of%20causal%C2%A0insights.html) about overinterpreting the results of feature importance!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "t2m2Y3NJIV5L",
    "outputId": "881367de-9213-46a1-f511-c7e6ce4543d1"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(forest.feature_importances_,\n",
    "                                   index = x.columns, \n",
    "                                   columns=['importance']).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBCrLvOcIV5L"
   },
   "source": [
    "#### Neural Network feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_t4hSM_RTtKl"
   },
   "source": [
    "Feature importance from Neural Nets is not so straightforward. The approach implemented below makes a prediction for each feature - setting each feature to equal 1 and the rest to 0, and repeating fo reach feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ki7egQHkUWtw",
    "outputId": "8a64b2f1-b35b-4dd9-f9d4-0f7e02cb6396"
   },
   "outputs": [],
   "source": [
    "df['temp_F']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bBiDtqdhIV5L",
    "outputId": "d0e26e80-3610-4896-e400-67e4439567ab"
   },
   "outputs": [],
   "source": [
    "cols = x.columns.values\n",
    "nn_featimportance = []\n",
    "for var in cols:\n",
    "    # create a vector corresponding to a 1 where the feature is located:\n",
    "    inputvector = np.array((cols==var).astype(int).reshape(1, -1))\n",
    "    nn_featimportance.append(nn.predict(inputvector)[0][0]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "dp66cpb1IV5L",
    "outputId": "8ccb270e-7a06-4ec3-c1cf-d61b243cd488"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame( nn_featimportance,\n",
    "             index = x.columns,\n",
    "             columns=['importance']).sort_values('importance',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2wGxV-sIV5L"
   },
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRC4e7bLIV5L"
   },
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1ansax_IV5L"
   },
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "vWSPnq5lIV5M",
    "outputId": "17ba7b91-ce7a-45c3-c2ca-5104ccc860c4"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/ATOC5860/ATOC5860_Spring2022_share/applicationlab6/')\n",
    "from solutions import supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYxg8uEPIV5M"
   },
   "source": [
    "1. __One important \"gotcha\" in ML is the order of data preparation. Why should one should perform the train-test split *before* feature scaling and rebalancing? *Hint: think about using a trained model for future predictions. Why perform a test-train split at all? Also, what data is considered when scaling a dataset?*__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJvPjdV3IV5M"
   },
   "outputs": [],
   "source": [
    "supervised.answer1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4Jit0opIV5M"
   },
   "source": [
    "2. __Why did we choose to use accuracy, recall, and predicted precipitation probability as a way to compare models?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n8uuVfVZIV5M"
   },
   "outputs": [],
   "source": [
    "supervised.answer2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzqcvdGuIV5M"
   },
   "source": [
    "3. __Now that you have some exposure to various models, what do you think might be some pros and cons regarding each model?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKPhMXKHIV5M"
   },
   "outputs": [],
   "source": [
    "supervised.answer3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPW3rYHLIV5M"
   },
   "source": [
    "4. __Collinearity, or non-zero correlation among features, results in a model that is overly complex, reduces the statistical significance of the fit of the model, and prevents one from correctly identifying the importance of features. Are there features included in these models that are collinear? You may assess this by running the code below. If so, how do you think we should address collinearity?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2acVB3pmIV5M"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m15\u001b[39m])\n\u001b[1;32m      2\u001b[0m ax \u001b[38;5;241m=\u001b[39m sns\u001b[38;5;241m.\u001b[39mheatmap(x\u001b[38;5;241m.\u001b[39mcorr(), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoolwarm\u001b[39m\u001b[38;5;124m'\u001b[39m, annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, linewidths\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, square\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, vmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,annot_kws\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m18\u001b[39m});\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mxticks(fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,rotation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m45\u001b[39m);\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=[15, 15])\n",
    "ax = sns.heatmap(x.corr(), cmap='coolwarm', annot=True, linewidths=0.2, square=True, vmin=-1, vmax=1,annot_kws={\"size\": 18});\n",
    "plt.xticks(fontsize=20,rotation=45);\n",
    "plt.yticks(fontsize=20,rotation=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-z_4qNKSIV5M"
   },
   "outputs": [],
   "source": [
    "supervised.answer4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OmVkVTKbIV5M"
   },
   "outputs": [],
   "source": [
    "# !python \"/content/drive/MyDrive/ATOC5860/ATOC5860_Spring2022_share/applicationlab6/collinearity_solution.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzH02cugg_eV"
   },
   "source": [
    "5. __Above, we selected on subset of the data to be the testing dataset, and the remaining data to be our training data. Can you think of any issues with this approach__?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9GgB97whVV3"
   },
   "outputs": [],
   "source": [
    "supervised.answer5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Byxh6bC8IV5N"
   },
   "source": [
    "6. __For fun:__\n",
    "    - __Try a different set of parameters for the random forest. How do the metrics change? Does the model overfit?__\n",
    "    - __Try playing around with the neural network to see what happens. You can change the optimizer, the batch size, the number of epochs, the activation function of the last layer, and the learning rate. How do the metrics change? Does the model overfit? Do you run into errors?__"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "8Mml7XyMIV47",
    "4j2h_GB6IV5D"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
